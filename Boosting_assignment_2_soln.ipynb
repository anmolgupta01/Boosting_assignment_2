{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4f25746",
   "metadata": {},
   "source": [
    "# Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b30b127",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a machine learning technique that belongs to the family of ensemble learning methods and is used for regression tasks. It is an extension of the more general Gradient Boosting Machine (GBM) algorithm, which can be applied to both classification and regression problems. Gradient Boosting Regression builds a series of decision trees sequentially, with each tree trying to correct the errors of the combined model.\n",
    "\n",
    "Here's a high-level overview of how Gradient Boosting Regression works:\n",
    "\n",
    "1. **Initialization:**\n",
    "   - The algorithm starts with an initial prediction, which is often set as the mean of the target values in the training data.\n",
    "\n",
    "2. **Residual Calculation:**\n",
    "   - The residuals (the differences between the actual and predicted values) are computed for each data point.\n",
    "\n",
    "3. **Train a Weak Learner (Decision Tree):**\n",
    "   - A decision tree is trained to predict the residuals. This tree is referred to as a \"weak learner\" because it is a simple model that is not expected to make accurate predictions on its own.\n",
    "\n",
    "4. **Update Predictions:**\n",
    "   - The predictions from the current weak learner are added to the ensemble model, adjusting the predictions toward a better fit.\n",
    "\n",
    "5. **Residual Update:**\n",
    "   - The residuals are recalculated based on the difference between the actual values and the current predictions.\n",
    "\n",
    "6. **Repeat:**\n",
    "   - Steps 3-5 are repeated for a predefined number of iterations or until a stopping criterion is met.\n",
    "\n",
    "7. **Final Prediction:**\n",
    "   - The final prediction is obtained by summing up the predictions from all the weak learners.\n",
    "\n",
    "Mathematically, the final prediction (\\(F(x)\\)) is given by:\n",
    "\n",
    "\\[ F(x) = \\sum_{i=1}^{N} h_i(x) \\]\n",
    "\n",
    "where \\(N\\) is the total number of weak learners, and \\(h_i(x)\\) is the prediction of the \\(i\\)-th weak learner.\n",
    "\n",
    "Gradient Boosting Regression uses the gradient of the loss function with respect to the current model's predictions to guide the training process. It updates the model in the direction that minimizes the loss, leading to a more accurate and robust regression model.\n",
    "\n",
    "Common implementations of Gradient Boosting Regression include libraries like Scikit-Learn's GradientBoostingRegressor, XGBoost, LightGBM, and CatBoost, each with its own optimizations and features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1090e2",
   "metadata": {},
   "source": [
    "# Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fb3d63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 6.14\n",
      "R-squared: 0.93\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize the model with the mean of the target variable\n",
    "        initial_prediction = np.mean(y)\n",
    "        self.models.append(initial_prediction)\n",
    "\n",
    "        # Fit weak learners (decision trees) to the residuals\n",
    "        residuals = y - initial_prediction\n",
    "        for _ in range(self.n_estimators):\n",
    "            tree = DecisionTreeRegressor(max_depth=3)\n",
    "            tree.fit(X, residuals)\n",
    "            prediction = tree.predict(X)\n",
    "            self.models.append(tree)\n",
    "            residuals -= self.learning_rate * prediction\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Make predictions using the ensemble of weak learners\n",
    "        predictions = np.zeros(len(X))\n",
    "        for model in self.models[1:]:\n",
    "            predictions += self.learning_rate * model.predict(X)\n",
    "\n",
    "        # Add the initial prediction (mean of target variable)\n",
    "        predictions += self.models[0]\n",
    "\n",
    "        return predictions\n",
    "\n",
    "# Create a synthetic dataset for demonstration\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 3 * X.squeeze() + np.random.randn(100) * 3\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the gradient boosting model\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = gb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse:.2f}')\n",
    "print(f'R-squared: {r2:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6989bb09",
   "metadata": {},
   "source": [
    "# Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db994792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50}\n",
      "Mean Squared Error: 5.99\n",
      "R-squared: 0.93\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Create a synthetic dataset for demonstration\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 3 * X.squeeze() + np.random.randn(100) * 3\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "# Initialize the Gradient Boosting Regressor\n",
    "gb_regressor = GradientBoostingRegressor()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(gb_regressor, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_gb_regressor = GradientBoostingRegressor(**best_params)\n",
    "best_gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_gb_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(f'Mean Squared Error: {mse:.2f}')\n",
    "print(f'R-squared: {r2:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a3a4b5",
   "metadata": {},
   "source": [
    "# Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bdf662",
   "metadata": {},
   "source": [
    "In the context of gradient boosting, a weak learner refers to a model that performs slightly better than random chance on a given problem. More specifically, it is a model that has limited predictive power on its own and may not be very accurate, but when combined with other weak learners in an ensemble, it contributes to the overall predictive performance of the model.\n",
    "\n",
    "In gradient boosting, the most commonly used weak learners are decision trees with limited depth. These shallow trees, often referred to as \"stumps\" when they have only a single split, are simple and capture only a small part of the overall patterns in the data. The reason for using weak learners in gradient boosting is that each tree is trained to correct the errors made by the preceding trees in the ensemble.\n",
    "\n",
    "During the training process of a gradient boosting algorithm, each weak learner is fitted to the residuals (the differences between the actual and predicted values) of the ensemble's current prediction. The subsequent weak learner is then trained to correct the errors left by the previous ones. This iterative process of adding weak learners sequentially and focusing on the mistakes made by the ensemble helps improve the model's predictive performance.\n",
    "\n",
    "The key idea behind gradient boosting is to combine a collection of weak learners to form a strong learner, where the \"strongness\" comes from the synergy of correcting errors collectively. By iteratively adding weak learners and adjusting their contributions, gradient boosting can build a highly accurate predictive model, especially effective in handling complex, non-linear relationships in data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d24e334",
   "metadata": {},
   "source": [
    "# Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5161b748",
   "metadata": {},
   "source": [
    "The Gradient Boosting algorithm is an ensemble learning technique that combines the predictions of multiple weak learners (typically decision trees) to create a strong predictive model. The intuition behind Gradient Boosting can be understood through the following key concepts:\n",
    "\n",
    "1. **Weak Learners:**\n",
    "   - Gradient Boosting uses a collection of weak learners, where each weak learner is a model that performs slightly better than random chance.\n",
    "   - Examples of weak learners often used in Gradient Boosting are shallow decision trees or decision stumps (trees with only one split).\n",
    "\n",
    "2. **Sequential Training:**\n",
    "   - The weak learners are added to the ensemble sequentially, one at a time.\n",
    "   - Each new weak learner is trained to correct the errors made by the combination of the existing ensemble.\n",
    "\n",
    "3. **Emphasis on Errors (Residuals):**\n",
    "   - At each iteration, the model focuses on the instances where the predictions are not accurate. It trains the new weak learner to predict the residuals (differences between the actual and predicted values) of the current ensemble.\n",
    "   - This emphasis on errors allows the model to gradually correct its mistakes and improve overall predictive accuracy.\n",
    "\n",
    "4. **Gradient Descent Optimization:**\n",
    "   - The term \"Gradient\" in Gradient Boosting comes from the optimization process, where the algorithm uses gradient descent to minimize a loss function.\n",
    "   - The loss function measures the difference between the model's predictions and the actual target values. Gradient Boosting aims to find the optimal direction (gradient) in the model's parameter space that minimizes this loss.\n",
    "\n",
    "5. **Weighted Combination of Weak Learners:**\n",
    "   - The predictions of the weak learners are combined in a weighted manner to form the final ensemble prediction.\n",
    "   - The weights are determined during the training process, and each weak learner's contribution is proportional to its ability to correct the errors of the ensemble.\n",
    "\n",
    "6. **Regularization through Shrinkage:**\n",
    "   - To prevent overfitting, a regularization technique called shrinkage is often applied. Shrinkage involves scaling the contribution of each weak learner by a small factor (learning rate).\n",
    "   - A lower learning rate makes the algorithm more robust and helps prevent the model from fitting the training data too closely.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662f225f",
   "metadata": {},
   "source": [
    "# Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c55c3a",
   "metadata": {},
   "source": [
    "The Gradient Boosting algorithm builds an ensemble of weak learners in a sequential manner. Here's a step-by-step explanation of how the ensemble is constructed:\n",
    "\n",
    "1. **Initialization:**\n",
    "   - The algorithm starts with an initial model, often a simple one like the mean or median of the target variable. This is the starting point for the ensemble.\n",
    "\n",
    "2. **Compute Residuals:**\n",
    "   - The initial model's predictions are subtracted from the actual target values to compute the residuals (the differences between the actual and predicted values).\n",
    "\n",
    "3. **Build a Weak Learner:**\n",
    "   - A weak learner, typically a decision tree with limited depth (a stump or a shallow tree), is trained on the dataset. The weak learner is fitted to the residuals rather than the original target values.\n",
    "\n",
    "4. **Compute Weighted Predictions:**\n",
    "   - The predictions of the weak learner are multiplied by a small learning rate. This is done to shrink the contribution of each weak learner, preventing overfitting and promoting better generalization.\n",
    "\n",
    "5. **Update Ensemble Predictions:**\n",
    "   - The weighted predictions of the weak learner are added to the predictions of the current ensemble. This updates the ensemble's overall predictions.\n",
    "\n",
    "6. **Compute New Residuals:**\n",
    "   - The residuals are recomputed by subtracting the updated ensemble predictions from the actual target values. The next weak learner is then trained on these new residuals.\n",
    "\n",
    "7. **Iterative Process:**\n",
    "   - Steps 3 to 6 are repeated for a specified number of iterations or until a certain criterion is met. At each iteration, a new weak learner is added to the ensemble, and its predictions are used to update the overall predictions of the model.\n",
    "\n",
    "8. **Final Ensemble Prediction:**\n",
    "   - The final prediction is obtained by summing up the weighted predictions of all weak learners in the ensemble.\n",
    "\n",
    "The key idea is that each weak learner is trained to correct the errors (residuals) made by the existing ensemble. This sequential process continues until the model converges or until a predetermined number of weak learners are added.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73ffe2a",
   "metadata": {},
   "source": [
    "# Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44124723",
   "metadata": {},
   "source": [
    "Constructing the mathematical intuition of the Gradient Boosting algorithm involves understanding how the algorithm optimizes a loss function using gradient descent and builds an ensemble of weak learners. Here are the key steps involved in the mathematical intuition of Gradient Boosting:\n",
    "\n",
    "1. **Loss Function:**\n",
    "   - Define a loss function that measures the difference between the model's predictions and the actual target values. Common loss functions for regression problems include Mean Squared Error (MSE) or Absolute Error (MAE).\n",
    "\n",
    "2. **Initialize Model:**\n",
    "   - Start with an initial model, often a simple one like the mean or median of the target variable. Let this initial prediction be denoted as \\( F_0(x) \\).\n",
    "\n",
    "3. **Compute Residuals:**\n",
    "   - Calculate the residuals by subtracting the actual target values from the current ensemble's predictions: \\( r_i = y_i - F_{\\text{ensemble}}(x_i) \\), where \\( r_i \\) is the residual for the \\( i \\)-th instance.\n",
    "\n",
    "4. **Build Weak Learner:**\n",
    "   - Train a weak learner (typically a decision tree) on the residuals. Denote the output of the weak learner as \\( h(x) \\).\n",
    "\n",
    "5. **Compute Weighted Predictions:**\n",
    "   - Multiply the predictions of the weak learner by a small learning rate (\\( \\alpha \\)) to control the contribution of each weak learner: \\( \\alpha h(x) \\).\n",
    "\n",
    "6. **Update Ensemble Predictions:**\n",
    "   - Update the ensemble predictions by adding the weighted predictions of the weak learner to the current ensemble: \\( F_{\\text{ensemble}}(x) = F_{\\text{ensemble}}(x) + \\alpha h(x) \\).\n",
    "\n",
    "7. **Compute New Residuals:**\n",
    "   - Recalculate the residuals using the updated ensemble predictions: \\( r_i = y_i - F_{\\text{ensemble}}(x_i) \\).\n",
    "\n",
    "8. **Iterative Process:**\n",
    "   - Repeat steps 4 to 7 for a specified number of iterations or until a convergence criterion is met. At each iteration, a new weak learner is added to the ensemble, and the ensemble predictions are updated.\n",
    "\n",
    "9. **Final Ensemble Prediction:**\n",
    "   - The final prediction is obtained by summing up the predictions of all weak learners in the ensemble: \\( F(x) = F_0(x) + \\sum_{i=1}^{N} \\alpha_i h_i(x) \\).\n",
    "\n",
    "10. **Gradient Descent Optimization:**\n",
    "    - The algorithm minimizes the loss function by using gradient descent. The negative gradient of the loss function with respect to the current ensemble's predictions guides the updates: \\( -\\frac{\\partial L(y, F_{\\text{ensemble}}(x))}{\\partial F_{\\text{ensemble}}(x)} \\).\n",
    "\n",
    "11. **Shrinkage for Regularization:**\n",
    "    - Introduce a shrinkage parameter (\\( \\eta \\)) that scales the contribution of each iteration. Smaller values of \\( \\eta \\) provide stronger regularization and help prevent overfitting.\n",
    "\n",
    "Understanding these steps provides a mathematical intuition for how Gradient Boosting optimizes a loss function, trains weak learners to correct errors, and builds an ensemble that improves predictive performance iteratively. The mathematical details involve computing derivatives of the loss function, and the optimization process is often implemented using techniques like gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3165087",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
